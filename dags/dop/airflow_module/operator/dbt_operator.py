import logging
import os
from typing import List, Dict

from airflow.contrib.hooks.gcs_hook import GoogleCloudStorageHook
from airflow.operators.bash_operator import BashOperator
from airflow.sensors.base_sensor_operator import apply_defaults
from dop.airflow_module.operator import dbt_operator_helper
from dop.component.configuration.env import env_config

# List of files generated by dbt docs generate
# https://docs.getdbt.com/reference/commands/cmd-docs
DBT_DOC_FILES = ["index.html", "manifest.json", "catalog.json"]
DBT_DOC_FOLDER = "target"
DBT_RUN_RESULTS_PATH = "target/run_results.json"


class DbtOperator(BashOperator):
    template_fields = (
        "action",
        "target",
        "dbt_project_path",
        "dbt_version",
        "dbt_arguments",
        "bash_command",
    )
    ui_color = "#FF694B"

    @apply_defaults
    def __init__(
        self,
        dbt_project_name: str,
        dbt_version: str,
        dbt_arguments: List[Dict],
        *args,
        **kwargs,
    ):
        """
        :param dbt_project_name: the name for the dbt project name inline with what's defined in `.dbt-project-repos.json`
        :param dbt_version: a supported DBT version, version must be >= 0.19.1
        :param args:
        :param kwargs: must contain the Task entity
        """

        task = kwargs["task"]
        self.dbt_project_path = os.path.sep.join(
            [env_config.dbt_projects_path, dbt_project_name]
        )
        self.dbt_project_name = dbt_project_name
        self.dbt_version = dbt_version
        self.action = task.kind.action
        self.target = task.kind.target
        self.dbt_arguments = dbt_arguments

        self._full_refresh = (
            False  # used to trigger DBT full refresh, modified via execute() override
        )

        super(DbtOperator, self).__init__(
            bash_command=self.parse_bash_command(), *args, **kwargs
        )

    def execute(self, context):
        """
        Override the parent method to ingest required contexts
        """
        dag_run_conf = context["dag_run"].conf if context["dag_run"].conf else {}
        full_refresh = dag_run_conf.get("full_refresh", False)

        self._full_refresh = full_refresh

        logging.info(f"### IS FULL REFRESH ENABLED: {self._full_refresh}")

        self.bash_command = self.parse_bash_command(context=context)
        super(DbtOperator, self).execute(context=context)

    def parse_bash_command(self, context=None):
        """
        Create a virtualenv and run DBT. Virtualenv is removed regardless if the script is successful or not
        """

        full_refresh_cmd = ""
        if self.target != "run":
            full_refresh_cmd = ""
        elif self.dbt_arguments:
            if self._full_refresh and "--full-refresh" not in [
                arg.get("option") for arg in self.dbt_arguments
            ]:
                full_refresh_cmd = "--full-refresh"
        elif self._full_refresh:
            full_refresh_cmd = "--full-refresh"

        set_err_handling = "set -xe"
        trap = """
        trap 'catch $? $LINENO' ERR
        catch() {
          echo "Script errored, removing virtualenv"
          rm -rf $TMP_DIR
          exit 1
        }
        """
        cmd_for_tmp_dir = "export TMP_DIR=$(mktemp -d)"
        cmd_to_print_tmp_dir = "echo TMP_DIR is: $TMP_DIR"
        cmd_for_virtualenv = "virtualenv -p python3 $TMP_DIR"
        dbt_init = f"PYTHONPATH={env_config.dag_path} python {env_config.dag_path}/dop/component/helper/dbt_init.py --tmp_dir=$TMP_DIR --project_name={self.dbt_project_name}"
        cmd_for_activating_virtualenv = "source $TMP_DIR/bin/activate"
        install_pip_deps = f"pip install dbt=={self.dbt_version}"

        cmd_for_additional_arguments = ""

        # docs arguments are only used to copy files to GCS, not in the task execution
        if self.dbt_arguments and self.target != "docs generate":
            cmd_for_additional_arguments = dbt_operator_helper.implode_arguments(
                dbt_arguments=self.dbt_arguments
            )

        cmd_to_run_dbt = (
            f"dbt clean --project-dir {self.dbt_project_path} --profiles-dir $TMP_DIR/.dbt"
            f" && dbt deps --project-dir {self.dbt_project_path}"
            f" && dbt --no-use-colors {self.target} --project-dir {self.dbt_project_path}"
            f" --profiles-dir $TMP_DIR/.dbt"
            f" --vars {dbt_operator_helper.parsed_cmd_airflow_context_vars(context=context)}"
            f" {cmd_for_additional_arguments}"
            f" {full_refresh_cmd}"
        )

        cmd_to_remove_tmp_dir = "rm -rf $TMP_DIR"

        return "\n".join(
            [
                set_err_handling,
                trap,
                cmd_for_tmp_dir,
                cmd_to_print_tmp_dir,
                cmd_for_virtualenv,
                dbt_init,  # setup dbt profiles.yml & service account secret from Secret Manager
                cmd_for_activating_virtualenv,
                install_pip_deps,
                cmd_to_run_dbt,
                cmd_to_remove_tmp_dir,
            ]
        )

    def post_execute(self, context, result=None):
        """
        This hook is triggered right after self.execute() is called.
        It is passed the execution context and any results returned by the
        operator.
        """
        if self.target == "docs generate":
            gcs_bucket = dbt_operator_helper.extract_argument(
                self.dbt_arguments, "--bucket"
            )
            if not gcs_bucket:
                logging.warning("No bucket argument provided. Skipping copy to GCS")
            gcs_path = dbt_operator_helper.extract_argument(
                self.dbt_arguments, "--bucket-path", ""
            )
            logging.info(f"Copying dbt docs JSON files to GCS bucket {gcs_bucket}")
            dbt_operator_helper.copy_docs_to_gcs(
                gcs_bucket, gcs_path, self.dbt_project_path
            )
        
        dbt_operator_helper.save_run_results_in_bq(
            env_config.project_id,
            self.dbt_project_name,
            f"{self.dbt_project_path}/{DBT_RUN_RESULTS_PATH}",
        )

    def copy_docs_to_gcs(self, bucket: str, bucket_path: str, project_path: str):
        """
        Copy doc files generated with dbt docs generate to GCS

        :param bucket: Bucket where the doc files will be copied
        :param bucket_path: Path in the bucket
        :param project_path: Local project folder
        """
        hook = GoogleCloudStorageHook()
        for doc_file in DBT_DOC_FILES:
            doc_file_path = f"{project_path}/{DBT_DOC_FOLDER}/{doc_file}"
            if os.path.exists(doc_file_path):
                logging.info(
                    f"{doc_file} found. Copying to gs://{bucket}/{bucket_path}"
                )
                hook.upload(
                    bucket,
                    object=f"{bucket_path}/{doc_file}" if bucket_path else doc_file,
                    filename=doc_file_path,
                    mime_type="text/html"
                    if doc_file.endswith(".html")
                    else "application/json",
                )
            else:
                logging.warning(f"{doc_file} not found. Skipping")
